# Local Model Deployment and Integration with LangChain

This repository showcases my experience with downloading and running three models locally, as well as integrating them with LangChain and other libraries. The models included are Llama3, Openchat, and Gemma.

## Models Deployed

1. **Llama3 Model**
2. **Openchat Model**
3. **Gemma Model**

## Project Structure

- `Llama3/` - Contains scripts and documentation for running the Llama3 model.
- `Openchat/` - Contains scripts and documentation for running the Openchat model.
- `Gemma/` - Contains scripts and documentation for running the Gemma model.
- `LangChain/` - Contains integration scripts and examples using LangChain with the above models.

## Prerequisites

Before running the models, ensure you have the following installed:

- Download the Ollama.
- Pull Llama3, Openchat, Gemma.
- Python 3.10.0
- Necessary libraries listed in `requirements.txt`

## Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/Hassanmustafa786/Ollama.git
    ```

2. Create a virtual environment and activate it:

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3. Install the required libraries:

    ```bash
    pip install -r requirements.txt
    ```

## Contact

If you have any questions or would like to discuss further, feel free to reach out to me at hassanqureshi700@gmail.com.

---

**Hafiz Hassan Mustafa**
